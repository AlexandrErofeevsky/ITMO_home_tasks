{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d87c3fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ed59ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksandrerofeevskij/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksandrerofeevskij/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aleksandrerofeevskij/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a49c79",
   "metadata": {},
   "source": [
    "# Download text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2cca459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "response = requests.get(link)\n",
    "original_text = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472956f3",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f590d",
   "metadata": {},
   "source": [
    "## Convert to lower case & remove non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d8a4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = original_text.lower()\n",
    "text = re.sub(r\"[^a-z ]\", \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048d63c",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba7ea66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_list = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8c80c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = [word for word in token_list if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d22061",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4cb3680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list = [lemmatizer.lemmatize(word) for word in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22af88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94911ea2",
   "metadata": {},
   "source": [
    "# Top 10 most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "34d724e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitted = text.split('chapter')\n",
    "text_split_by_chapters = text_splitted[13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a9dfb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', analyzer='word').fit(text_split_by_chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "95be1b59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words = []\n",
    "for chapter_text in text_split_by_chapters:\n",
    "    vectorizer = TfidfVectorizer(input='content', analyzer='word')\n",
    "        \n",
    "    matr = vectorizer.fit_transform([chapter_text])\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    top_words_by_chapter = [features[i] for i in matr.sum(axis=0).argsort()[0, -11:][0]]\n",
    "    top_words.append(top_words_by_chapter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a1738bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 could, nothing, door, thought, think, way, one, see, like, little\n",
      "2 im, one, must, went, foot, dear, thing, said, mouse, little\n",
      "3 question, lory, one, soon, long, thing, know, dodo, mouse, said\n",
      "4 voice, get, bill, thought, heard, quite, one, rabbit, said, little\n",
      "5 bit, dont, serpent, ive, size, pigeon, im, little, caterpillar, said\n",
      "6 would, baby, went, little, much, footman, duchess, like, cat, said\n",
      "7 say, went, thing, know, time, hare, march, dormouse, hatter, said\n",
      "8 began, went, three, two, see, cat, king, head, queen, said\n",
      "9 moral, say, dont, queen, went, gryphon, duchess, turtle, mock, said\n",
      "10 could, join, lobster, beautiful, wont, would, gryphon, turtle, mock, said\n",
      "11 witness, thought, court, rabbit, dormouse, queen, one, hatter, king, said\n",
      "12 king, state, copy, term, electronic, gutenberg, foundation, gutenbergtm, said, work\n"
     ]
    }
   ],
   "source": [
    "for i, chapter_top_words in enumerate(top_words):\n",
    "    print(i+1, \", \".join([word for word in chapter_top_words[0] if word != 'alice'][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeef57f",
   "metadata": {},
   "source": [
    "## Predicted names\n",
    "1. The thinking door\n",
    "2. Little mouse must go\n",
    "3. Lory's soon long thing\n",
    "4. Talking rabbit\n",
    "5. Serpent, pigeon and caterpilalr\n",
    "6. Little duchess's footman\n",
    "7. The hatter teaches the hare\n",
    "8. The king and the queen\n",
    "9. The moral\n",
    "10. The gryphon and the turtle\n",
    "11. Hatter-witness\n",
    "12. King's copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859c0a9",
   "metadata": {},
   "source": [
    "## Original names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "749bf590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Down the Rabbit-Hole\n",
      "2. The Pool of Tears\n",
      "3. A Caucus-Race and a Long Tale\n",
      "4. The Rabbit Sends in a Little Bill\n",
      "5. Advice from a Caterpillar\n",
      "6. Pig and Pepper\n",
      "7. A Mad Tea-Party\n",
      "8. The Queenâs Croquet-Ground\n",
      "9. The Mock Turtleâs Story\n",
      "10. The Lobster Quadrille\n",
      "11. Who Stole the Tarts?\n",
      "12. Aliceâs Evidence\n"
     ]
    }
   ],
   "source": [
    "chapters_names = original_text.split('CHAPTER')[1:13]\n",
    "\n",
    "for i, chapter_name in enumerate(chapters_names):\n",
    "    print(f\"{i+1}. {' '.join((chapter_name.split()[1:]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffe2ab8",
   "metadata": {},
   "source": [
    "# Top 10 verbs with Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "68172f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = original_text.lower().split(\".\")\n",
    "all_sentences = [re.sub(r\"[^a-z ]\", \"\", sentence) for sentence in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c61e8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "aba03423",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in all_sentences:\n",
    "    token_list = word_tokenize(sentence)\n",
    "    token_list = [word for word in token_list if word not in stopwords.words('english')]\n",
    "    token_list = [lemmatizer.lemmatize(word) for word in token_list]\n",
    "    prepared_sentences.append(\" \".join(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "070acedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_verbs = {}\n",
    "for sentence in prepared_sentences:\n",
    "    if 'alice' in sentence:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        words_with_type = nltk.pos_tag(tokens)\n",
    "        verbs = [word for word, type_word in words_with_type if type_word == 'VB']\n",
    "        for verb in verbs:\n",
    "            if verb in alice_verbs:\n",
    "                alice_verbs[verb] += 1\n",
    "            else:\n",
    "                alice_verbs[verb] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7be011a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'restrictionswhatsoever': 1,\n",
       " 'get': 11,\n",
       " 'thehot': 1,\n",
       " 'worth': 1,\n",
       " 'take': 3,\n",
       " 'see': 16,\n",
       " 'let': 5,\n",
       " 'toget': 1,\n",
       " 'belong': 1,\n",
       " 'open': 1,\n",
       " 'go': 15,\n",
       " 'little': 1,\n",
       " 'find': 3,\n",
       " 'drink': 2,\n",
       " 'say': 7,\n",
       " 'wise': 1,\n",
       " 'sort': 1,\n",
       " 'shrink': 1,\n",
       " 'end': 1,\n",
       " 'reachit': 1,\n",
       " 'come': 14,\n",
       " 'use': 4,\n",
       " 'make': 3,\n",
       " 'feel': 2,\n",
       " 'manage': 1,\n",
       " 'bekind': 1,\n",
       " 'ill': 3,\n",
       " 'give': 3,\n",
       " 'tear': 1,\n",
       " 'glove': 3,\n",
       " 'beenchanged': 2,\n",
       " 'improve': 1,\n",
       " 'tailand': 1,\n",
       " 'clawsand': 1,\n",
       " 'mabel': 1,\n",
       " 'andi': 1,\n",
       " 'stay': 2,\n",
       " 'look': 2,\n",
       " 'im': 1,\n",
       " 'put': 1,\n",
       " 'declare': 1,\n",
       " 'rightway': 1,\n",
       " 'offended': 1,\n",
       " 'talk': 1,\n",
       " 'hear': 5,\n",
       " 'kill': 1,\n",
       " 'tell': 8,\n",
       " 'know': 3,\n",
       " 'allowwithout': 1,\n",
       " 'catch': 1,\n",
       " 'tospeak': 1,\n",
       " 'think': 3,\n",
       " 'please': 1,\n",
       " 'happen': 4,\n",
       " 'dinah': 1,\n",
       " 'stop': 1,\n",
       " 'fanciedshe': 1,\n",
       " 'thought': 1,\n",
       " 'roof': 1,\n",
       " 'burn': 1,\n",
       " 'found': 1,\n",
       " 'eat': 2,\n",
       " 'keep': 5,\n",
       " 'hold': 2,\n",
       " 'seehow': 1,\n",
       " 'begin': 1,\n",
       " 'knowand': 1,\n",
       " 'shecould': 1,\n",
       " 'wait': 1,\n",
       " 'remember': 5,\n",
       " 'injure': 1,\n",
       " 'lookout': 1,\n",
       " 'need': 1,\n",
       " 'neck': 1,\n",
       " 'kept': 1,\n",
       " 'sense': 1,\n",
       " 'sneezing': 1,\n",
       " 'grin': 1,\n",
       " 'introduce': 1,\n",
       " 'advantage': 1,\n",
       " 'cook': 1,\n",
       " 'abidefigures': 1,\n",
       " 'nurse': 1,\n",
       " 'child': 1,\n",
       " 'doubt': 1,\n",
       " 'hada': 1,\n",
       " 'denied': 1,\n",
       " 'right': 1,\n",
       " 'fun': 1,\n",
       " 'wellsay': 1,\n",
       " 'wonder': 1,\n",
       " 'like': 3,\n",
       " 'tosome': 1,\n",
       " 'draw': 1,\n",
       " 'bear': 1,\n",
       " 'call': 2,\n",
       " 'herthe': 1,\n",
       " 'lie': 1,\n",
       " 'play': 2,\n",
       " 'twist': 1,\n",
       " 'helpbursting': 1,\n",
       " 'become': 1,\n",
       " 'somebody': 1,\n",
       " 'goingon': 1,\n",
       " 'hasnt': 1,\n",
       " 'love': 1,\n",
       " 'try': 1,\n",
       " 'bite': 1,\n",
       " 'tofeel': 1,\n",
       " 'quite': 1,\n",
       " 'break': 1,\n",
       " 'nothelp': 1,\n",
       " 'ashamed': 1,\n",
       " 'speak': 1,\n",
       " 'beautify': 1,\n",
       " 'lived': 1,\n",
       " 'pretty': 1,\n",
       " 'believe': 1,\n",
       " 'comewrong': 1,\n",
       " 'stoopsoup': 1,\n",
       " 'pas': 1,\n",
       " 'ask': 1,\n",
       " 'stand': 1,\n",
       " 'hedid': 1,\n",
       " 'tart': 1}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba06fa",
   "metadata": {},
   "source": [
    "Usually Alice goes and sees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
