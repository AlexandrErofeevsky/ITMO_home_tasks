{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87c3fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed59ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandrerofeevskij/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksandrerofeevskij/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksandrerofeevskij/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aleksandrerofeevskij/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a49c79",
   "metadata": {},
   "source": [
    "# Download text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cca459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "response = requests.get(link)\n",
    "original_text = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472956f3",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f590d",
   "metadata": {},
   "source": [
    "## Convert to lower case & remove non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8a4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = original_text.lower()\n",
    "text = re.sub(r\"[^a-z ]\", \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048d63c",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7ea66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_list = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c80c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = [word for word in token_list if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d22061",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb3680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list = [lemmatizer.lemmatize(word) for word in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22af88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94911ea2",
   "metadata": {},
   "source": [
    "# Top 10 most important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d724e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitted = text.split('chapter')\n",
    "text_split_by_chapters = text_splitted[13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9dfb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', analyzer='word').fit(text_split_by_chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95be1b59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words = []\n",
    "for chapter_text in text_split_by_chapters:\n",
    "    matr = vectorizer.transform([chapter_text])\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    top_words_by_chapter = [features[i] for i in matr.sum(axis=0).argsort()[0, -11:][0]]\n",
    "    top_words.append(top_words_by_chapter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a242aa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rabbit, way, one, see, door, eat, like, little, key, bat\n",
      "2 mabel, oh, cried, said, cat, dear, swam, pool, little, mouse\n",
      "3 dinah, know, course, thimble, dry, prize, lory, dodo, said, mouse\n",
      "4 window, ann, one, fan, said, bottle, little, bill, rabbit, puppy\n",
      "5 im, caterpillarwell, father, size, egg, youth, serpent, pigeon, caterpillar, said\n",
      "6 much, mad, cook, like, pig, duchess, baby, footman, cat, said\n",
      "7 time, asleep, draw, tea, twinkle, hare, march, said, hatter, dormouse\n",
      "8 five, head, executioner, hedgehog, gardener, soldier, cat, king, said, queen\n",
      "9 school, dont, went, queen, moral, duchess, gryphon, turtle, said, mock\n",
      "10 soup, wont, whiting, beautiful, join, lobster, said, gryphon, turtle, mock\n",
      "11 march, rabbit, thecourt, juror, queen, dormouse, court, witness, hatter, said\n",
      "12 copyright, said, copy, state, term, gutenberg, electronic, foundation, work, gutenbergtm\n"
     ]
    }
   ],
   "source": [
    "for i, chapter_top_words in enumerate(top_words):\n",
    "    print(i+1, \", \".join([word for word in chapter_top_words[0] if word != 'alice'][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeef57f",
   "metadata": {},
   "source": [
    "## Predicted names\n",
    "1. The rabbit sees the door\n",
    "2. Cat speak with little mouse\n",
    "3. Dodo and the mouse\n",
    "4. The window\n",
    "5. Speaking animals\n",
    "6. Baby pig\n",
    "7. Time to sleep\n",
    "8. Executioner with five heads\n",
    "9. It's better than school\n",
    "10. Lobster's soup\n",
    "11. The rabbit's march\n",
    "12. Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859c0a9",
   "metadata": {},
   "source": [
    "## Original names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "749bf590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Down the Rabbit-Hole\n",
      "2. The Pool of Tears\n",
      "3. A Caucus-Race and a Long Tale\n",
      "4. The Rabbit Sends in a Little Bill\n",
      "5. Advice from a Caterpillar\n",
      "6. Pig and Pepper\n",
      "7. A Mad Tea-Party\n",
      "8. The Queenâs Croquet-Ground\n",
      "9. The Mock Turtleâs Story\n",
      "10. The Lobster Quadrille\n",
      "11. Who Stole the Tarts?\n",
      "12. Aliceâs Evidence\n"
     ]
    }
   ],
   "source": [
    "chapters_names = original_text.split('CHAPTER')[1:13]\n",
    "\n",
    "for i, chapter_name in enumerate(chapters_names):\n",
    "    print(f\"{i+1}. {' '.join((chapter_name.split()[1:]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffe2ab8",
   "metadata": {},
   "source": [
    "# Top 10 verbs with Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68172f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = original_text.lower().split(\".\")\n",
    "all_sentences = [re.sub(r\"[^a-z ]\", \"\", sentence) for sentence in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c61e8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aba03423",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in all_sentences:\n",
    "    token_list = word_tokenize(sentence)\n",
    "    token_list = [word for word in token_list if word not in stopwords.words('english')]\n",
    "    token_list = [lemmatizer.lemmatize(word) for word in token_list]\n",
    "    prepared_sentences.append(\" \".join(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "070acedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_verbs = {}\n",
    "for sentence in prepared_sentences:\n",
    "    if 'alice' in sentence:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        words_with_type = nltk.pos_tag(tokens)\n",
    "        verbs = [word for word, type_word in words_with_type if type_word[:2] == 'VB']\n",
    "        for verb in verbs:\n",
    "            if verb in alice_verbs:\n",
    "                alice_verbs[verb] += 1\n",
    "            else:\n",
    "                alice_verbs[verb] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d2ef015",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_alice_verbs = {k: v for k, v in sorted(alice_verbs.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26528e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice said 199 times\n",
      "Alice went 39 times\n",
      "Alice thought 36 times\n",
      "Alice say 31 times\n",
      "Alice go 26 times\n",
      "Alice know 25 times\n",
      "Alice see 24 times\n",
      "Alice got 23 times\n",
      "Alice began 23 times\n",
      "Alice looked 22 times\n"
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(sorted_alice_verbs.items()):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(f\"Alice {k} {v} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba06fa",
   "metadata": {},
   "source": [
    "Usually Alice speaks and goes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
